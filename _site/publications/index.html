<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Publications | Eugy  Han</title>
    <meta name="author" content="Eugy  Han" />
    <meta name="description" content="Journal publications and conference proceedings in reversed chronological order." />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>👓</text></svg>">
    
    <link rel="stylesheet" href="/eugyoung.github.io/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/eugyoung.github.io/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/eugyoung.github.io/assets/js/theme.js"></script>
    <script src="/eugyoung.github.io/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000/eugyoung.github.io//"><span class="font-weight-bold">Eugy</span>   Han</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/eugyoung.github.io/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/eugyoung.github.io/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/eugyoung.github.io/teaching/">Teaching</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/eugyoung.github.io/cv/">CV</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description">Journal publications and conference proceedings in reversed chronological order.</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2024</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview rounded" src="/eugyoung.github.io/assets/img/publication_preview/han_ceilingheight.jpg"></div>

        <!-- Entry bib key -->
        <div id="han_ceilingheight_2024" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">The influence of spatial dimensions of virtual environments on attitudes and nonverbal behaviors during social interactions,</div>
          <!-- Title -->
          <div class="authors"></div>
          <!-- Author -->
          <div class="author">
          
                  <em>Han, Eugy</em>, DeVeaux, Cyan, Hancock, Jeffrey T., Ram, Nilam, Harari, Gabriella M.,  and 
                Bailenson, Jeremy N.

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Journal of Environmental Psychology</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/eugyoung.github.io/assets/pdf/han_spatialdimensions.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Research on physical-world environments has shown that the spatial properties of built worlds are consequential for shaping psychological states and social behavior. However, it has been difficult to empirically test this in natural settings in the physical world. This study uses immersive virtual reality (VR) environments, which have shown to have comparable effects to physical-world environments, to investigate the influence of two spatial dimensions (ceiling height and floor area) on individuals’ attitudes and nonverbal behaviors during social interactions. In the present study, groups of three to four physically remote participants wore VR headsets (n = 10) and took part in discussions every week for four weeks in one of four virtual environments that varied in their spatial dimensions (low or high ceilings, small or large floor areas). Results showed that, when in a virtual environment with a high ceiling, participants reported feeling greater perceived restorativeness, awe, and momentary affective well-being, compared to when they were in virtual environments with low ceilings. Participants also paid more social attention (i.e., looked at other group members), when they were in virtual environments with high ceilings. When in a virtual environment with a large floor area, participants reported having a greater sense of awe, compared to environments with small floor areas. Furthermore, when in a large environment with a high ceiling, participants physically moved their heads more slowly and virtually stood further apart from their group members, compared to the other three conditions. We discuss implications for theoretical work on context and behaviors as well as design of social VR environments.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview rounded" src="/eugyoung.github.io/assets/img/publication_preview/wang_DS_2024.jpg"></div>

        <!-- Entry bib key -->
        <div id="wang_understanding_2024" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Understanding virtual design behaviors: A large-scale analysis of the design process in Virtual Reality</div>
          <!-- Title -->
          <div class="authors"></div>
          <!-- Author -->
          <div class="author">
          Wang, Portia., Miller, Mark R., 
                  <em>Han, Eugy</em>, DeVeaux, Cyan.,  and 
                Bailenson, Jeremy N.

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Design Studies</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/eugyoung.github.io/assets/pdf/wang_designstudies.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Virtual Reality (VR) is an emerging medium with consequences for studying design processes. In VR, users can design using direct manipulation and move both by walking and using their hands in the physical world and beyond physical spaces using abstract movement such as teleportation. However, research examining VR design processes remains limited. In this work, we present a large-scale analysis of 730 VR designs from 254 students. We built models of VR design processes, selecting features based on previous theoretical and empirical research. By examining these models at scale, we analyzed design behaviors and their relationship with the context and final design. This research provides a tool for describing VR design processes and highlights broader implications for designers and educators.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview rounded" src="/eugyoung.github.io/assets/img/publication_preview/akselrad_bodyCrumple_2023.png"></div>

        <!-- Entry bib key -->
        <div id="akselrad_body_2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Body Crumple, Sound Intrusion, and Embodiment Violation: Toward a Framework for Miscommunication in VR</div>
          <!-- Title -->
          <div class="authors"></div>
          <!-- Author -->
          <div class="author">
          Akselrad, Daniel, DeVeaux, Cyan, 
                  <em>Han, Eugy</em>, Miller, Mark R.,  and 
                Bailenson, Jeremy N.

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ACM Computer-Supported Cooperative Work</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/eugyoung.github.io/assets/pdf/akselrad_bodycrumple.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The advent of widely-accessible VR has enabled individuals to communicate—and miscommunicate—in new ways. To explore these miscommunications, we introduce a preliminary framework based on events that occurred during 3600 minutes of observation inside a university course taught in VR. During the course, 250 people met in groups for about 20 minutes per session. We identify three types of miscommunication that routinely occurred: body crumple, sound intrusion, and embodiment violation. By mapping the affordances by which social VR fails to facilitate effective communication, we hope to provide educators, developers, and virtual ethnographers with a means for understanding and navigating the challenges of VR-based collaboration.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview rounded" src="/eugyoung.github.io/assets/img/publication_preview/deveaux_discrepancy_2023.jpg"></div>

        <!-- Entry bib key -->
        <div id="deveaux_exploring_2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Exploring the Relationship Between Attribute Discrepancy and Avatar Embodiment in Immersive Social Virtual Reality</div>
          <!-- Title -->
          <div class="authors"></div>
          <!-- Author -->
          <div class="author">
          DeVeaux, Cyan, 
                  <em>Han, Eugy</em>, Landay, James A.,  and 
                Bailenson, Jeremy N.

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Cyberpsychology, Behavior, and Social Networking</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/eugyoung.github.io/assets/pdf/deveaux_avatardiscrepancy.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Social virtual reality (VR) is an emerging set of platforms where users interact while embodying avatars. Given that VR headsets track real physical movements and map them onto one’s avatar body, the nature of one’s digital representation is an important aspect of social VR. However, little is known about how the visual proximity of an avatar to the self shapes user experience in naturalistic, social VR environments. In this article, we use this context to explore how embodiment is influenced by the perceived differences between the physical attributes of a user and the virtual attributes of their avatar. We selected a number of attributes for this measure that have been shown to be important for customization and representation in VR. Participants created an avatar, spent time in social VR, and reported on their experience in a questionnaire. Our results demonstrate a significant negative association between attribute discrepancy and avatar embodiment, the psychological experience of one’s virtual body as their own body. We discuss implications for theories of self-representation and suggest urgency on the part of games and VR designers to improve the methods of creating avatars.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview rounded" src="/eugyoung.github.io/assets/img/publication_preview/deveaux_language_2023.png"></div>

        <!-- Entry bib key -->
        <div id="deveaux_descriptive_2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Descriptive Linguistic Patterns of Group Conversations in VR</div>
          <!-- Title -->
          <div class="authors"></div>
          <!-- Author -->
          <div class="author">
          DeVeaux, Cyan, Markowitz, Mark D., 
                  <em>Han, Eugy</em>, Miller, Mark R., Hancock, Jeffrey T.,  and 
                N. Bailenson, Jeremy N.

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IEEE Virtual Reality and 3D User Interfaces Abstracts and Workshops</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/eugyoung.github.io/assets/pdf/deveaux_language.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Although talking is one of the most common activities in social VR, there is little empirical work identifying what people say and how they communicate in immersive, virtual settings. The current paper addresses this opportunity by performing automated text analysis on over 4,800 minutes of in-Vr, small group conversations. These conversations took place over the span of two months during a university course where 171 students attended discussion sections via head-mounted displays. We provide a methodology for analyzing verbal communication along two dimensions: content and structure. We implement methods to describe linguistic patterns from the class and introduce a preliminary VR Dictionary.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview rounded" src="/eugyoung.github.io/assets/img/publication_preview/han_peopleplaces_2023.png"></div>

        <!-- Entry bib key -->
        <div id="han_people_2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">People, places, and time: a large-scale, longitudinal study of transformed avatars and environmental context in group interaction in the metaverse</div>
          <!-- Title -->
          <div class="authors"></div>
          <!-- Author -->
          <div class="author">
          
                  <em>Han, Eugy</em>, Miller, Mark R., DeVeaux, Cyan, Jun, Hanseul., Nowak, Kristine L., Hancock, Jeffrey T., Ram, Nilam,  and 
                Bailenson, Jeremy N.

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Journal of Computer-Mediated Communication</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/eugyoung.github.io/assets/pdf/han_peopleplaces.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>As the metaverse expands, understanding how people use virtual reality to learn and connect is increasingly important. We used the Transformed Social Interaction paradigm (Bailenson et al., 2004) to examine different avatar identities and environments over time. In Study 1 (n = 81), entitativity, presence, enjoyment, and realism increased over 8 weeks. Avatars that resembled participants increased synchrony, similarities in moment-to-moment nonverbal behaviors between participants. Moreover, self-avatars increased self-presence and realism, but decreased enjoyment, compared to uniform avatars. In Study 2 (n = 137), participants cycled through 192 unique virtual environments. As visible space increased, so did nonverbal synchrony, perceived restorativeness, entitativity, pleasure, arousal, self- and spatial presence, enjoyment, and realism. Outdoor environments increased perceived restorativeness and enjoyment more than indoor environments. Self-presence and realism increased over time in both studies. We discuss implications of avatar appearance and environmental context on social behavior in classroom contexts over time.Understanding how people connect socially via avatars in immersive virtual reality has become increasingly important given the prolific rise of the metaverse. In two large-scale, longitudinal field experiments, we extended predictions of the Transformed Social Interaction paradigm to investigate how the appearance of avatars and the characteristics of the virtual environment influenced people’s behaviors and attitudes over time. In Study 1, we demonstrated the effects of time: group cohesion, presence, enjoyment, and realism measures increased over time, and the effects of appearance: When represented by avatars that looked like themselves, people displayed more synchronous nonverbal behaviors, or were more “in sync” with others, and reported the image quality of the environment and people as more realistic. On the other hand, when people wore the same uniform avatar, they experienced more enjoyment. In Study 2, we demonstrated the effects of the environment: When in more spacious virtual environments, there was more synchronous movement and people reported feeling greater restoration, group cohesion, pleasure, arousal, presence, enjoyment, and realism, than in constrained environments. When in outdoor environments with elements of nature, people reported feeling greater restoration and enjoyment than in indoor environments.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview rounded" src="/eugyoung.github.io/assets/img/publication_preview/miller_proxemics_2023.png"></div>

        <!-- Entry bib key -->
        <div id="r_miller_large-scale_2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">A Large-Scale Study of Proxemics and Gaze in Groups</div>
          <!-- Title -->
          <div class="authors"></div>
          <!-- Author -->
          <div class="author">
          Miller, Mark R., DeVeaux, Cyan, 
                  <em>Han, Eugy</em>, Ram, Nilam,  and 
                Bailenson, Jeremy N.

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IEEE Virtual Reality and 3D User Interfaces</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/eugyoung.github.io/assets/pdf/miller_proxemics.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Scholars who study nonverbal behavior have focused an incredible amount of work on proxemics, how close people stand to one another, and mutual gaze, whether or not they are looking at one another. Moreover, many studies have demonstrated a correlation between gaze and distance, and so-called equilibrium theory posits that people modulate gaze and distance to maintain proper levels of nonverbal intimacy. Virtual reality scholars have also focused on these two constructs, both for theoretical reasons, as distance and gaze are often used as proxies for psychological constructs such as social presence, and for methodological reasons, as head orientation and body position are automatically produced by most VR tracking systems. However, to date, the studies of distance and gaze in VR have largely been conducted in laboratory settings, observing behavior of a small number of participants for short periods of time. In this experimental field study, we analyze the proxemics and gaze of 232 participants over two experimental studies who each contributed up to about 240 minutes of tracking data during eight weekly 30-minute social virtual reality sessions. Participants’ non-verbal behaviors changed in conjunction with context manipulations and over time. Interpersonal distance increased with the size of the virtual room; and both mutual gaze and interpersonal distance increased over time. Overall, participants oriented their heads toward the center of walls rather than to corners of rectangularly-aligned environments. Finally, statistical models demonstrated that individual differences matter, with pairs and groups maintaining more consistent differences over time than would be predicted by chance. Implications for theory and practice are discussed.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview rounded" src="/eugyoung.github.io/assets/img/publication_preview/queiroz_cscl_2023.png"></div>

        <!-- Entry bib key -->
        <div id="queiroz_collaborative_2023" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Collaborative Tasks in Immersive Virtual Reality Increase Learning</div>
          <!-- Title -->
          <div class="authors"></div>
          <!-- Author -->
          <div class="author">
          Queiroz, Anna C.M., McGivney, Eileen, Liu, Sunny X., Anderson, Courtney, Beams, Brian, DeVeaux, Cyan, Frazier, Kai, 
                  <em>Han, Eugy</em>, Miller, Mark R., Peterson, Xander S., Woolsey, Erika S., Hancock, Jeffrey T.,  and 
                Bailenson, Jeremy N.

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 16th International Conference on Computer-Supported Collaborative Learning</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/eugyoung.github.io/assets/pdf/queiroz_learningivr.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Advances in immersive virtual reality (IVR) are creating more computer-supported collaborative learning environments, but there is little research explicating how collaboration in IVR impacts learning. We ran a quasi-experimental study with 80 participants targeting ocean literacy learning, varying the manner in which participants interacted in IVR to investigate how the design of collaborative IVR experiences influences learning. Results are discussed through the lens of collaborative cognitive learning theory. Participants that collaborated to actively build a new environment in IVR scored higher for learning than participants who only watched an instructional guide’s avatar, or participants who watched the guide’s avatar and subsequently discussed what they learned while in IVR. Moreover, feeling negative emotions, feeling active in the environment, and feeling bonded to the group members negatively correlated with learning. Results shed light on the mechanisms behind how collaborative tasks in IVR can support learning.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview rounded" src="/eugyoung.github.io/assets/img/publication_preview/han_tmb_2022.png"></div>

        <!-- Entry bib key -->
        <div id="han_prerequisites_2022" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Prerequisites for Learning in Networked Immersive Virtual Reality</div>
          <!-- Title -->
          <div class="authors"></div>
          <!-- Author -->
          <div class="author">
          
                  <em>Han, Eugy</em>, Nowak, Kristine L.,  and 
                Bailenson, Jeremy N.

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Technology, Mind, and Behavior</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/eugyoung.github.io/assets/pdf/han_prerequisiteslearning.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>There has been growing interest in using virtual reality (VR) as a solution for many of the challenges facing distance education, such as fostering a sense of connectedness with classmates. However, implementing VR in distance education has its share of challenges, such as hardware accessibility and a scarcity of content which match curricula. In this exploratory, mixed-methods study, we examined 19 students’ use of head-mounted displays to meet with classmates inside social VR. For 4 weeks, students worked together in small groups on various tasks inside a virtual environment. We present quantitative results on attitudes foundational to fostering ideal learning environments. Entitativity (“group-ness”), enjoyment, realism, and presence did not change over time, likely due to a small sample size resulting from technical difficulties in collecting data. We present qualitative observations on instructors’ and students’ experiences across time and with VR use, and how these may inform curricula development. First, it is critical to provide ample training time to allow students to grow accustomed to the medium before investigating how response to VR changes over time. Without learning how to use VR first, students cannot learn inside VR. Second, we discuss task type and content considerations within and outside of VR and provide recommendations on how to reduce cognitive load and encourage social interaction. Third, we address technological and social issues that are likely to arise. Overall, we focus on ways to create a sense of connectedness and reduce psychological distance and challenges that may disrupt meaningful interactions from taking place.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview rounded" src="/eugyoung.github.io/assets/img/publication_preview/amit_corruption_2020.png"></div>

        <!-- Entry bib key -->
        <div id="amit_how_2020" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">How People Judge Institutional Corruption</div>
          <!-- Title -->
          <div class="authors"></div>
          <!-- Author -->
          <div class="author">
          Amit, Elinor, 
                  <em>Han, Eugy</em>, Posten, Ann-Christin,  and 
                Sloman, Steven

          

          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Connecticut Law Review</em>
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/eugyoung.github.io/assets/pdf/han_institutionalcorruption.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Institutional corruption refers to actions that are legal yet carry negative consequences for the greater good. Such legal yet harmful behaviors have been observed among politicians and donors who establish quid-pro-quo relationships in exchange for money and among doctors who receive gifts from pharmaceutical companies in return for recommending the companies’ drugs. How does the general public reconcile the tension between the legal status of an action and its impact on the greater good and judge the action’s moral acceptability? We explored this question empirically by comparing the relative weight people give to the legal status of actions and to the impact of actions when judging moral acceptability. Results show that people unequivocally rely on legal status and ignore the impact of the actions. We conclude that people outsource their moral judgments to the law. The law does not simply reflect people’s sense of corruption but determines it. Together, our research suggests a surprising and ironic role for the law: that it diminishes independent, critical thinking.</p>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Eugy  Han. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://eugyoung.github.io/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/eugyoung.github.io/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/eugyoung.github.io/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/eugyoung.github.io/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
